"use strict";(globalThis.webpackChunkhumanoid_robotics_book=globalThis.webpackChunkhumanoid_robotics_book||[]).push([[4438],{7164:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>t,default:()=>h,frontMatter:()=>o,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"ai-robot-brain/vslam","title":"VSLAM (Visual SLAM)","description":"Chapter 5:","source":"@site/docs/03-ai-robot-brain/05-vslam.md","sourceDirName":"03-ai-robot-brain","slug":"/ai-robot-brain/vslam","permalink":"/hackathon-1/docs/ai-robot-brain/vslam","draft":false,"unlisted":false,"editUrl":"https://github.com/FareaaFaisal/hackathon-1/edit/main/docs/03-ai-robot-brain/05-vslam.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"sidebar_position":5,"title":"VSLAM (Visual SLAM)"},"sidebar":"tutorialSidebar","previous":{"title":"Isaac ROS Perception","permalink":"/hackathon-1/docs/ai-robot-brain/isaac-ros-perception"},"next":{"title":"Navigation for Bipedal Robots","permalink":"/hackathon-1/docs/ai-robot-brain/bipedal-navigation"}}');var r=i(4848),a=i(8453);const o={sidebar_position:5,title:"VSLAM (Visual SLAM)"},t=void 0,l={},c=[{value:"Chapter 5:",id:"chapter-5",level:2},{value:"5.1 Principles of Visual SLAM",id:"51-principles-of-visual-slam",level:2},{value:"5.2 VSLAM Algorithms",id:"52-vslam-algorithms",level:2},{value:"5.3 Implementing VSLAM in Isaac Sim",id:"53-implementing-vslam-in-isaac-sim",level:2},{value:"Step 1: Configure Camera Sensors",id:"step-1-configure-camera-sensors",level:3},{value:"Step 2: Launch VSLAM Nodes",id:"step-2-launch-vslam-nodes",level:3},{value:"Step 3: Map Creation",id:"step-3-map-creation",level:3},{value:"Step 4: Localization",id:"step-4-localization",level:3},{value:"Step 5: Visualization",id:"step-5-visualization",level:3},{value:"5.4 Applications in Humanoid Robotics",id:"54-applications-in-humanoid-robotics",level:2},{value:"5.5 Best Practices",id:"55-best-practices",level:2},{value:"5.6 Summary",id:"56-summary",level:2},{value:"5.7 Learning Outcomes",id:"57-learning-outcomes",level:2},{value:"References",id:"references",level:2}];function d(e){const n={a:"a",blockquote:"blockquote",code:"code",em:"em",h2:"h2",h3:"h3",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.h2,{id:"chapter-5",children:"Chapter 5:"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Visual SLAM (VSLAM)"})," is the process of simultaneously ",(0,r.jsx)(n.strong,{children:"creating a map of an environment"})," and ",(0,r.jsx)(n.strong,{children:"localizing the robot"})," within it using only visual sensor data, typically from cameras or RGB-D sensors. For humanoid robots, VSLAM enables autonomous navigation in dynamic and unknown environments."]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"51-principles-of-visual-slam",children:"5.1 Principles of Visual SLAM"}),"\n",(0,r.jsx)(n.p,{children:"VSLAM integrates several key components:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Feature Extraction:"})," Detecting and tracking features (e.g., corners, edges, keypoints) in camera images."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Pose Estimation:"})," Determining the camera's position and orientation relative to the environment."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Map Building:"})," Creating a 2D or 3D map of the environment using tracked features."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Loop Closure:"})," Recognizing previously visited locations to correct accumulated drift."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Sensor Fusion:"})," Optionally combining IMU, LiDAR, or depth data for more robust localization."]}),"\n"]}),"\n",(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Definition:"})," ",(0,r.jsx)(n.em,{children:"Visual SLAM"}),": The process of using visual information from cameras to simultaneously map an environment and determine a robot\u2019s pose within it."]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"52-vslam-algorithms",children:"5.2 VSLAM Algorithms"}),"\n",(0,r.jsx)(n.p,{children:"Common algorithms for VSLAM include:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"ORB-SLAM3:"})," Uses ORB features for real-time tracking and mapping."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"RTAB-Map:"})," Creates 3D occupancy maps with loop closure detection."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"LSD-SLAM:"})," Direct method using image intensity information instead of feature extraction."]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["Each algorithm balances ",(0,r.jsx)(n.strong,{children:"accuracy"}),", ",(0,r.jsx)(n.strong,{children:"speed"}),", and ",(0,r.jsx)(n.strong,{children:"robustness"})," depending on the application and computational resources."]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"53-implementing-vslam-in-isaac-sim",children:"5.3 Implementing VSLAM in Isaac Sim"}),"\n",(0,r.jsx)(n.h3,{id:"step-1-configure-camera-sensors",children:"Step 1: Configure Camera Sensors"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Attach RGB or RGB-D cameras to the humanoid robot model."}),"\n",(0,r.jsx)(n.li,{children:"Set resolution, frame rate, and field of view consistent with the robot's real-world sensors."}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"step-2-launch-vslam-nodes",children:"Step 2: Launch VSLAM Nodes"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Use Isaac ROS VSLAM or compatible ROS 2 packages:"}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:'node:\r\n  name: vslam_node\r\n  type: isaac_ros_vslam/VSLAMNode\r\n  parameters:\r\n    camera_topic: "/camera/color/image_raw"\r\n    camera_info_topic: "/camera/color/camera_info"\r\n    map_topic: "/vslam/map"\r\n    pose_topic: "/vslam/pose"\n'})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Verify that camera calibration parameters are accurate to ensure proper depth estimation and feature tracking."}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"step-3-map-creation",children:"Step 3: Map Creation"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Move the humanoid robot through the environment."}),"\n",(0,r.jsx)(n.li,{children:"Features are detected and tracked in real-time."}),"\n",(0,r.jsx)(n.li,{children:"A 2D or 3D map is incrementally built."}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"step-4-localization",children:"Step 4: Localization"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["The robot continuously estimates its ",(0,r.jsx)(n.strong,{children:"pose relative to the generated map"}),"."]}),"\n",(0,r.jsx)(n.li,{children:"Loop closure helps correct drift when revisiting known areas."}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"step-5-visualization",children:"Step 5: Visualization"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:["Use ",(0,r.jsx)(n.strong,{children:"RViz2"})," or Isaac Sim visualizers to view:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Robot trajectory."}),"\n",(0,r.jsx)(n.li,{children:"Map point clouds or occupancy grids."}),"\n",(0,r.jsx)(n.li,{children:"Feature correspondences and loop closures."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"54-applications-in-humanoid-robotics",children:"5.4 Applications in Humanoid Robotics"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Autonomous navigation in indoor and outdoor environments."}),"\n",(0,r.jsx)(n.li,{children:"Safe human-robot interaction with mapped workspaces."}),"\n",(0,r.jsx)(n.li,{children:"Task execution requiring precise spatial awareness, such as object manipulation or delivery tasks."}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"55-best-practices",children:"5.5 Best Practices"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Lighting Conditions:"})," Ensure consistent illumination to avoid feature loss."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Camera Calibration:"})," Accurate intrinsic and extrinsic calibration is critical."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Realistic Simulation:"})," Include dynamic obstacles and human avatars to mimic real-world conditions."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Data Logging:"})," Record camera and robot poses for offline validation and improvement."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Hardware Acceleration:"})," Utilize GPU acceleration when possible for real-time performance."]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"56-summary",children:"5.6 Summary"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["VSLAM enables ",(0,r.jsx)(n.strong,{children:"simultaneous mapping and localization"})," using visual sensors."]}),"\n",(0,r.jsx)(n.li,{children:"Isaac Sim and Isaac ROS provide tools to simulate VSLAM with humanoid robots."}),"\n",(0,r.jsxs)(n.li,{children:["Proper configuration, calibration, and validation are key for ",(0,r.jsx)(n.strong,{children:"accurate map creation"})," and ",(0,r.jsx)(n.strong,{children:"real-time localization"}),"."]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"57-learning-outcomes",children:"5.7 Learning Outcomes"}),"\n",(0,r.jsx)(n.p,{children:"After completing these steps, students will be able to:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["Implement VSLAM for real-time ",(0,r.jsx)(n.strong,{children:"map creation"}),"."]}),"\n",(0,r.jsxs)(n.li,{children:["Perform ",(0,r.jsx)(n.strong,{children:"robot localization"})," relative to the generated map."]}),"\n",(0,r.jsxs)(n.li,{children:["Visualize VSLAM outputs in ",(0,r.jsx)(n.strong,{children:"RViz2"})," or Isaac Sim."]}),"\n",(0,r.jsxs)(n.li,{children:["Apply best practices for ",(0,r.jsx)(n.strong,{children:"robust VSLAM performance"})," in simulation or real environments."]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"references",children:"References"}),"\n",(0,r.jsxs)(n.p,{children:["[1] Mur-Artal, R., et al., \u201cORB-SLAM3: An Accurate Open-Source Library for Visual, Visual-Inertial and Multi-Map SLAM,\u201d ",(0,r.jsx)(n.em,{children:"IEEE Transactions on Robotics"}),", 2021."]}),"\n",(0,r.jsxs)(n.p,{children:["[2] Labbe, M., et al., \u201cRTAB-Map: Real-Time Appearance-Based Mapping,\u201d ",(0,r.jsx)(n.em,{children:"Autonomous Robots"}),", 2019."]}),"\n",(0,r.jsxs)(n.p,{children:["[3] Engel, J., et al., \u201cLSD-SLAM: Large-Scale Direct Monocular SLAM,\u201d ",(0,r.jsx)(n.em,{children:"ECCV"}),", 2014."]}),"\n",(0,r.jsxs)(n.p,{children:["[4] NVIDIA, \u201cIsaac Sim Documentation \u2013 VSLAM,\u201d ",(0,r.jsx)(n.em,{children:(0,r.jsx)(n.a,{href:"https://developer.nvidia.com/isaac-sim",children:"https://developer.nvidia.com/isaac-sim"})}),"."]}),"\n",(0,r.jsxs)(n.p,{children:["[5] Siciliano, B., et al., ",(0,r.jsx)(n.em,{children:"Springer Handbook of Robotics"}),", 2nd ed., Springer, 2016."]}),"\n",(0,r.jsx)(n.hr,{})]})}function h(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>o,x:()=>t});var s=i(6540);const r={},a=s.createContext(r);function o(e){const n=s.useContext(a);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function t(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:o(e.components),s.createElement(a.Provider,{value:n},e.children)}}}]);