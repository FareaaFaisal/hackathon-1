"use strict";(globalThis.webpackChunkhumanoid_robotics_book=globalThis.webpackChunkhumanoid_robotics_book||[]).push([[1822],{3577:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>r,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"vision-language-action/vlm","title":"Vision-Language Models","description":"Chapter 4:","source":"@site/docs/04-vision-language-action/04-vlm.md","sourceDirName":"04-vision-language-action","slug":"/vision-language-action/vlm","permalink":"/hackathon-1/docs/vision-language-action/vlm","draft":false,"unlisted":false,"editUrl":"https://github.com/FareaaFaisal/hackathon-1/edit/main/docs/04-vision-language-action/04-vlm.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"sidebar_position":4,"title":"Vision-Language Models"},"sidebar":"tutorialSidebar","previous":{"title":"LLM Planning","permalink":"/hackathon-1/docs/vision-language-action/llm-planning"},"next":{"title":"Multimodal Pipeline","permalink":"/hackathon-1/docs/vision-language-action/multimodal-pipeline"}}');var t=i(4848),o=i(8453);const r={sidebar_position:4,title:"Vision-Language Models"},a=void 0,l={},c=[{value:"Chapter 4:",id:"chapter-4",level:2},{value:"4.1 Overview",id:"41-overview",level:2},{value:"4.2 VLM Architectures",id:"42-vlm-architectures",level:2},{value:"4.3 Scene Understanding",id:"43-scene-understanding",level:2},{value:"4.4 Integration with Robotics",id:"44-integration-with-robotics",level:2},{value:"4.5 Best Practices",id:"45-best-practices",level:2},{value:"4.6 Applications",id:"46-applications",level:2},{value:"4.7 Summary",id:"47-summary",level:2},{value:"4.8 Learning Outcomes",id:"48-learning-outcomes",level:2},{value:"References",id:"references",level:2}];function d(n){const e={blockquote:"blockquote",em:"em",h2:"h2",hr:"hr",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,o.R)(),...n.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.h2,{id:"chapter-4",children:"Chapter 4:"}),"\n",(0,t.jsxs)(e.p,{children:["Vision-Language Models (VLMs) integrate ",(0,t.jsx)(e.strong,{children:"visual perception"})," and ",(0,t.jsx)(e.strong,{children:"natural language understanding"}),", enabling robots to identify, describe, and interact with objects in their environment. These models form a core component of the ",(0,t.jsx)(e.strong,{children:"Vision-Language-Action (VLA) pipeline"}),", bridging the gap between what a robot sees and how it interprets instructions."]}),"\n",(0,t.jsx)(e.hr,{}),"\n",(0,t.jsx)(e.h2,{id:"41-overview",children:"4.1 Overview"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:["VLMs combine ",(0,t.jsx)(e.strong,{children:"computer vision models"})," (CNNs, Transformers) with ",(0,t.jsx)(e.strong,{children:"language models"})," (LLMs) to create multimodal representations."]}),"\n",(0,t.jsxs)(e.li,{children:["They allow robots to perform tasks like ",(0,t.jsx)(e.strong,{children:"object recognition, scene description, and grounding instructions in perception"}),"."]}),"\n",(0,t.jsx)(e.li,{children:"Applications include humanoid manipulation, autonomous navigation, and interactive AI systems."}),"\n"]}),"\n",(0,t.jsxs)(e.blockquote,{children:["\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Definition:"})," ",(0,t.jsx)(e.em,{children:"Vision-Language Model (VLM)"}),": A model that jointly processes visual data and textual information to interpret, reason, and act on real-world scenes."]}),"\n"]}),"\n",(0,t.jsx)(e.hr,{}),"\n",(0,t.jsx)(e.h2,{id:"42-vlm-architectures",children:"4.2 VLM Architectures"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Visual Encoder:"})," Extracts features from images or video frames using CNNs, ViTs, or ResNets."]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Language Encoder:"})," Converts text instructions or labels into embeddings."]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Multimodal Fusion:"})," Aligns visual and textual features using attention mechanisms or cross-modal transformers."]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Prediction Head:"})," Produces outputs such as object locations, segmentation masks, or action-relevant descriptors."]}),"\n"]}),"\n",(0,t.jsx)(e.hr,{}),"\n",(0,t.jsx)(e.h2,{id:"43-scene-understanding",children:"4.3 Scene Understanding"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:["Robots leverage VLMs to identify ",(0,t.jsx)(e.strong,{children:"objects, their attributes, and spatial relationships"})," in real time."]}),"\n",(0,t.jsxs)(e.li,{children:["Commands like ",(0,t.jsx)(e.em,{children:"\u201cPick up the red cube next to the green sphere\u201d"})," are grounded into ",(0,t.jsx)(e.strong,{children:"visual context"}),", allowing the robot to map instructions to physical objects."]}),"\n",(0,t.jsxs)(e.li,{children:["VLMs can also provide ",(0,t.jsx)(e.strong,{children:"descriptive captions"})," of the environment for higher-level reasoning or planning."]}),"\n"]}),"\n",(0,t.jsx)(e.hr,{}),"\n",(0,t.jsx)(e.h2,{id:"44-integration-with-robotics",children:"4.4 Integration with Robotics"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Perception Nodes:"})," VLM outputs are published to ROS 2 topics for downstream consumption."]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Planner Interface:"})," Identified objects and locations are used by VLA or ROS 2 planners to generate motion commands."]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Action Execution:"})," Robot executes tasks based on visual grounding and interpreted instructions."]}),"\n"]}),"\n",(0,t.jsxs)(e.blockquote,{children:["\n",(0,t.jsx)(e.p,{children:(0,t.jsx)(e.strong,{children:"Example:"})}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsxs)(e.li,{children:["Robot receives the command: ",(0,t.jsx)(e.em,{children:"\u201cPlace the blue block on the yellow tray.\u201d"})]}),"\n",(0,t.jsx)(e.li,{children:"VLM identifies blue block and yellow tray in the camera frame."}),"\n",(0,t.jsx)(e.li,{children:"ROS 2 planner generates trajectory for pick-and-place."}),"\n",(0,t.jsx)(e.li,{children:"Humanoid robot executes the action using motion control nodes."}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(e.hr,{}),"\n",(0,t.jsx)(e.h2,{id:"45-best-practices",children:"4.5 Best Practices"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"High-Quality Data:"})," Train or fine-tune VLMs on datasets relevant to your robot\u2019s environment."]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Real-Time Constraints:"})," Optimize inference for low latency on edge devices like NVIDIA Jetson."]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Multimodal Feedback:"})," Combine VLM outputs with depth and segmentation for accurate grasping."]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Robustness:"})," Handle occlusion, lighting variation, and dynamic objects."]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Testing:"})," Evaluate in simulation before deployment on physical hardware."]}),"\n"]}),"\n",(0,t.jsx)(e.hr,{}),"\n",(0,t.jsx)(e.h2,{id:"46-applications",children:"4.6 Applications"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Object recognition for manipulation and assembly tasks."}),"\n",(0,t.jsx)(e.li,{children:"Scene description for autonomous navigation and reasoning."}),"\n",(0,t.jsx)(e.li,{children:"Human-robot interaction where natural language refers to physical objects."}),"\n",(0,t.jsx)(e.li,{children:"Multimodal perception for digital twin environments."}),"\n"]}),"\n",(0,t.jsx)(e.hr,{}),"\n",(0,t.jsx)(e.h2,{id:"47-summary",children:"4.7 Summary"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:["VLMs enable ",(0,t.jsx)(e.strong,{children:"robots to connect visual perception with natural language commands"}),"."]}),"\n",(0,t.jsxs)(e.li,{children:["Integration with ROS 2 and VLA pipelines allows ",(0,t.jsx)(e.strong,{children:"context-aware scene understanding"}),"."]}),"\n",(0,t.jsxs)(e.li,{children:["Following best practices ensures ",(0,t.jsx)(e.strong,{children:"robust, real-time object identification and interaction"}),"."]}),"\n"]}),"\n",(0,t.jsx)(e.hr,{}),"\n",(0,t.jsx)(e.h2,{id:"48-learning-outcomes",children:"4.8 Learning Outcomes"}),"\n",(0,t.jsx)(e.p,{children:"After completing this chapter, students will be able to:"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsx)(e.li,{children:"Understand the architecture and capabilities of Vision-Language Models (VLMs)."}),"\n",(0,t.jsx)(e.li,{children:"Implement VLMs for scene understanding and object identification."}),"\n",(0,t.jsx)(e.li,{children:"Integrate VLM outputs into ROS 2 pipelines for robotic action execution."}),"\n",(0,t.jsx)(e.li,{children:"Enable robots to identify and interact with objects based on natural language commands."}),"\n",(0,t.jsx)(e.li,{children:"Optimize multimodal perception for real-time humanoid applications."}),"\n"]}),"\n",(0,t.jsx)(e.hr,{}),"\n",(0,t.jsx)(e.h2,{id:"references",children:"References"}),"\n",(0,t.jsxs)(e.p,{children:["[1] Radford, A., et al., \u201cLearning Transferable Visual Models From Natural Language Supervision,\u201d ",(0,t.jsx)(e.em,{children:"ICML"}),", 2021."]}),"\n",(0,t.jsxs)(e.p,{children:["[2] Li, X., et al., \u201cBLIP: Bootstrapping Language-Image Pretraining for Unified Vision-Language Understanding,\u201d ",(0,t.jsx)(e.em,{children:"ICCV"}),", 2023."]}),"\n",(0,t.jsxs)(e.p,{children:["[3] OpenAI, \u201cCLIP: Connecting Vision and Language,\u201d ",(0,t.jsx)(e.em,{children:"arXiv preprint"}),", 2021."]}),"\n",(0,t.jsxs)(e.p,{children:["[4] Tan, H., et al., \u201cVLMs for Robotic Manipulation,\u201d ",(0,t.jsx)(e.em,{children:"IEEE Robotics and Automation Letters"}),", 2022."]}),"\n",(0,t.jsxs)(e.p,{children:["[5] Shridhar, M., et al., \u201cInteractive VLMs for Human-Robot Interaction,\u201d ",(0,t.jsx)(e.em,{children:"ICRA"}),", 2022."]}),"\n",(0,t.jsx)(e.hr,{})]})}function h(n={}){const{wrapper:e}={...(0,o.R)(),...n.components};return e?(0,t.jsx)(e,{...n,children:(0,t.jsx)(d,{...n})}):d(n)}},8453:(n,e,i)=>{i.d(e,{R:()=>r,x:()=>a});var s=i(6540);const t={},o=s.createContext(t);function r(n){const e=s.useContext(o);return s.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function a(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(t):n.components||t:r(n.components),s.createElement(o.Provider,{value:e},n.children)}}}]);