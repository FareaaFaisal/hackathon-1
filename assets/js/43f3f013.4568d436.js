"use strict";(globalThis.webpackChunkhumanoid_robotics_book=globalThis.webpackChunkhumanoid_robotics_book||[]).push([[3087],{657:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>a,contentTitle:()=>l,default:()=>h,frontMatter:()=>o,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"vision-language-action/llm-planning","title":"LLM Planning","description":"Chapter 3:","source":"@site/docs/04-vision-language-action/03-llm-planning.md","sourceDirName":"04-vision-language-action","slug":"/vision-language-action/llm-planning","permalink":"/hackathon-1/docs/vision-language-action/llm-planning","draft":false,"unlisted":false,"editUrl":"https://github.com/FareaaFaisal/hackathon-1/edit/main/docs/04-vision-language-action/03-llm-planning.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3,"title":"LLM Planning"},"sidebar":"tutorialSidebar","previous":{"title":"Whisper for Speech Input","permalink":"/hackathon-1/docs/vision-language-action/whisper-input"},"next":{"title":"Vision-Language Models","permalink":"/hackathon-1/docs/vision-language-action/vlm"}}');var r=i(4848),t=i(8453);const o={sidebar_position:3,title:"LLM Planning"},l=void 0,a={},c=[{value:"Chapter 3:",id:"chapter-3",level:2},{value:"3.1 Overview",id:"31-overview",level:2},{value:"3.2 Workflow for LLM Planning",id:"32-workflow-for-llm-planning",level:2},{value:"3.3 Integration with ROS 2",id:"33-integration-with-ros-2",level:2},{value:"3.4 Best Practices",id:"34-best-practices",level:2},{value:"3.5 Applications",id:"35-applications",level:2},{value:"3.6 Summary",id:"36-summary",level:2},{value:"3.7 Learning Outcomes",id:"37-learning-outcomes",level:2},{value:"References",id:"references",level:2}];function d(n){const e={blockquote:"blockquote",br:"br",code:"code",em:"em",h2:"h2",hr:"hr",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,t.R)(),...n.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(e.h2,{id:"chapter-3",children:"Chapter 3:"}),"\n",(0,r.jsxs)(e.p,{children:["Large Language Models (LLMs) offer an advanced approach to converting natural language instructions into executable robotic actions. By bridging ",(0,r.jsx)(e.strong,{children:"language understanding"})," and ",(0,r.jsx)(e.strong,{children:"robotic execution"}),", LLMs enable humanoid robots to perform complex tasks without manually coding each behavior."]}),"\n",(0,r.jsx)(e.hr,{}),"\n",(0,r.jsx)(e.h2,{id:"31-overview",children:"3.1 Overview"}),"\n",(0,r.jsxs)(e.p,{children:["LLMs are pre-trained on vast text corpora, enabling them to ",(0,r.jsx)(e.strong,{children:"understand intent, generate structured outputs, and sequence actions"}),". In robotics, LLMs can:"]}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["Parse natural language commands into ",(0,r.jsx)(e.strong,{children:"structured plans"}),"."]}),"\n",(0,r.jsxs)(e.li,{children:["Sequence tasks into ",(0,r.jsx)(e.strong,{children:"ROS 2 nodes, services, or actions"}),"."]}),"\n",(0,r.jsx)(e.li,{children:"Provide reasoning or error handling instructions to improve robustness."}),"\n"]}),"\n",(0,r.jsxs)(e.blockquote,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Definition:"})," ",(0,r.jsx)(e.em,{children:"LLM-Based Planning"}),": Using a language model to convert natural language commands into a structured series of executable actions for a robotic system."]}),"\n"]}),"\n",(0,r.jsx)(e.hr,{}),"\n",(0,r.jsx)(e.h2,{id:"32-workflow-for-llm-planning",children:"3.2 Workflow for LLM Planning"}),"\n",(0,r.jsxs)(e.ol,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Input:"})," Natural language command, e.g., ",(0,r.jsx)(e.code,{children:'"Pick up the red cube and place it on the table."'})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Parsing:"})," LLM identifies actions, objects, and constraints."]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Plan Generation:"})," Produces a ",(0,r.jsx)(e.strong,{children:"step-by-step execution plan"})," compatible with ROS 2 topics, services, and action servers."]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Execution:"})," ROS 2 nodes execute each step sequentially or in parallel depending on dependencies."]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Feedback:"})," Robot status and environment observations are fed back to the LLM for adaptive planning."]}),"\n"]}),"\n",(0,r.jsx)(e.hr,{}),"\n",(0,r.jsx)(e.h2,{id:"33-integration-with-ros-2",children:"3.3 Integration with ROS 2"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Actions and Topics:"})," LLM-generated plans are converted to ROS 2 actions for motion execution."]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Service Calls:"})," LLM may invoke services for decision-making or state queries."]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Error Handling:"})," LLM interprets sensor feedback to dynamically adjust task execution."]}),"\n"]}),"\n",(0,r.jsxs)(e.blockquote,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Example:"}),(0,r.jsx)(e.br,{}),"\n","Command: ",(0,r.jsx)(e.em,{children:"\u201cMove the left arm to pick up the green ball and place it in the box.\u201d"}),(0,r.jsx)(e.br,{}),"\n","LLM Output:"]}),"\n",(0,r.jsxs)(e.ol,{children:["\n",(0,r.jsxs)(e.li,{children:["Identify green ball location \u2192 ROS topic ",(0,r.jsx)(e.code,{children:"/camera/objects"})]}),"\n",(0,r.jsxs)(e.li,{children:["Plan trajectory using MoveIt 2 \u2192 ROS action ",(0,r.jsx)(e.code,{children:"/move_group"})]}),"\n",(0,r.jsxs)(e.li,{children:["Close gripper \u2192 ROS service ",(0,r.jsx)(e.code,{children:"/gripper_control"})]}),"\n",(0,r.jsxs)(e.li,{children:["Move to box \u2192 ROS action ",(0,r.jsx)(e.code,{children:"/move_group"})]}),"\n",(0,r.jsxs)(e.li,{children:["Open gripper \u2192 ROS service ",(0,r.jsx)(e.code,{children:"/gripper_control"})]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(e.hr,{}),"\n",(0,r.jsx)(e.h2,{id:"34-best-practices",children:"3.4 Best Practices"}),"\n",(0,r.jsxs)(e.ol,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Preprocessing Commands:"})," Normalize language and correct spelling errors for accurate LLM interpretation."]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Context Awareness:"})," Provide LLM with environmental and robot state context."]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Safety Constraints:"})," Enforce motion limits and collision avoidance within the planning step."]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Feedback Loop:"})," Continuously monitor execution and adjust plans dynamically."]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Validation:"})," Test generated plans in simulation before real-world execution."]}),"\n"]}),"\n",(0,r.jsx)(e.hr,{}),"\n",(0,r.jsx)(e.h2,{id:"35-applications",children:"3.5 Applications"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Humanoid Manipulation:"})," Pick-and-place, assembly, and object rearrangement tasks."]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Service Robotics:"})," Household or healthcare robots executing multi-step instructions."]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Autonomous Research Platforms:"})," Robots interpreting natural language instructions in experimental setups."]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Human-Robot Collaboration:"})," LLM interprets high-level instructions from human operators into actionable plans."]}),"\n"]}),"\n",(0,r.jsx)(e.hr,{}),"\n",(0,r.jsx)(e.h2,{id:"36-summary",children:"3.6 Summary"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["LLMs enable ",(0,r.jsx)(e.strong,{children:"translation of natural language commands into actionable ROS 2 plans"}),"."]}),"\n",(0,r.jsx)(e.li,{children:"Integration requires mapping LLM outputs to ROS 2 nodes, services, and action servers."}),"\n",(0,r.jsxs)(e.li,{children:["Proper context, safety, and feedback mechanisms ensure ",(0,r.jsx)(e.strong,{children:"reliable humanoid execution"}),"."]}),"\n"]}),"\n",(0,r.jsx)(e.hr,{}),"\n",(0,r.jsx)(e.h2,{id:"37-learning-outcomes",children:"3.7 Learning Outcomes"}),"\n",(0,r.jsx)(e.p,{children:"After completing this chapter, students will be able to:"}),"\n",(0,r.jsxs)(e.ol,{children:["\n",(0,r.jsx)(e.li,{children:"Understand the role of LLMs in robotic planning."}),"\n",(0,r.jsx)(e.li,{children:"Convert natural language commands into structured step-by-step ROS 2 actions."}),"\n",(0,r.jsx)(e.li,{children:"Integrate LLM-based plans with humanoid robot pipelines."}),"\n",(0,r.jsx)(e.li,{children:"Implement dynamic feedback loops to adapt plans based on execution results."}),"\n",(0,r.jsx)(e.li,{children:"Validate generated plans in simulation before real-world execution."}),"\n"]}),"\n",(0,r.jsx)(e.hr,{}),"\n",(0,r.jsx)(e.h2,{id:"references",children:"References"}),"\n",(0,r.jsxs)(e.p,{children:["[1] Chen, L., et al., \u201cLanguage-Conditioned Imitation Learning for Robot Manipulation,\u201d ",(0,r.jsx)(e.em,{children:"IEEE Robotics and Automation Letters"}),", 2021."]}),"\n",(0,r.jsxs)(e.p,{children:["[2] Shridhar, M., et al., \u201cVIMA: General Robot Manipulation with Multi-Modal Prompts,\u201d ",(0,r.jsx)(e.em,{children:"ICRA"}),", 2022."]}),"\n",(0,r.jsxs)(e.p,{children:["[3] Bisk, Y., et al., \u201cActionable Language in Robotics,\u201d ",(0,r.jsx)(e.em,{children:"Robotics and Autonomous Systems"}),", 2020."]}),"\n",(0,r.jsxs)(e.p,{children:["[4] OpenAI, \u201cCLIP: Connecting Vision and Language,\u201d ",(0,r.jsx)(e.em,{children:"arXiv preprint"}),", 2021."]}),"\n",(0,r.jsxs)(e.p,{children:["[5] Kress-Gazit, H., et al., ",(0,r.jsx)(e.em,{children:"Principles of Robot Motion: Theory, Algorithms, and Implementations"}),", MIT Press, 2022."]}),"\n",(0,r.jsx)(e.hr,{})]})}function h(n={}){const{wrapper:e}={...(0,t.R)(),...n.components};return e?(0,r.jsx)(e,{...n,children:(0,r.jsx)(d,{...n})}):d(n)}},8453:(n,e,i)=>{i.d(e,{R:()=>o,x:()=>l});var s=i(6540);const r={},t=s.createContext(r);function o(n){const e=s.useContext(t);return s.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function l(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(r):n.components||r:o(n.components),s.createElement(t.Provider,{value:e},n.children)}}}]);