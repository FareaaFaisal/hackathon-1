"use strict";(globalThis.webpackChunkhumanoid_robotics_book=globalThis.webpackChunkhumanoid_robotics_book||[]).push([[8482],{8453:(e,n,t)=>{t.d(n,{R:()=>a,x:()=>o});var s=t(6540);const i={},r=s.createContext(i);function a(e){const n=s.useContext(r);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:a(e.components),s.createElement(r.Provider,{value:n},e.children)}},9768:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>a,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"ai-robot-brain/synthetic-data","title":"Synthetic Data","description":"Chapter 3:","source":"@site/docs/03-ai-robot-brain/03-synthetic-data.md","sourceDirName":"03-ai-robot-brain","slug":"/ai-robot-brain/synthetic-data","permalink":"/hackathon-1/docs/ai-robot-brain/synthetic-data","draft":false,"unlisted":false,"editUrl":"https://github.com/FareaaFaisal/hackathon-1/edit/main/docs/03-ai-robot-brain/03-synthetic-data.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3,"title":"Synthetic Data"},"sidebar":"tutorialSidebar","previous":{"title":"Isaac Sim for Humanoid Robots","permalink":"/hackathon-1/docs/ai-robot-brain/isaac-sim-humanoids"},"next":{"title":"Isaac ROS Perception","permalink":"/hackathon-1/docs/ai-robot-brain/isaac-ros-perception"}}');var i=t(4848),r=t(8453);const a={sidebar_position:3,title:"Synthetic Data"},o=void 0,l={},c=[{value:"Chapter 3:",id:"chapter-3",level:2},{value:"3.1 Importance of Synthetic Data",id:"31-importance-of-synthetic-data",level:2},{value:"3.2 Types of Synthetic Labels",id:"32-types-of-synthetic-labels",level:2},{value:"3.3 Generating Synthetic Data in Isaac Sim",id:"33-generating-synthetic-data-in-isaac-sim",level:2},{value:"Step 1: Setup Simulation Scene",id:"step-1-setup-simulation-scene",level:3},{value:"Step 2: Enable Ground Truth Output",id:"step-2-enable-ground-truth-output",level:3},{value:"Step 3: Record Synthetic Data",id:"step-3-record-synthetic-data",level:3},{value:"Step 4: Export Dataset",id:"step-4-export-dataset",level:3},{value:"3.4 Applications of Synthetic Data",id:"34-applications-of-synthetic-data",level:2},{value:"3.5 Best Practices",id:"35-best-practices",level:2},{value:"3.6 Summary",id:"36-summary",level:2},{value:"3.7 Learning Outcomes",id:"37-learning-outcomes",level:2},{value:"References",id:"references",level:2}];function d(e){const n={a:"a",blockquote:"blockquote",code:"code",em:"em",h2:"h2",h3:"h3",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.h2,{id:"chapter-3",children:"Chapter 3:"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Synthetic data"})," refers to artificially generated data used to train and validate machine learning models. In robotics, Isaac Sim provides tools to generate ",(0,i.jsx)(n.strong,{children:"high-fidelity labeled data"})," for perception tasks, including ",(0,i.jsx)(n.strong,{children:"segmentation, depth estimation, and 2D/3D keypoints"}),", without requiring physical robots or sensors."]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"31-importance-of-synthetic-data",children:"3.1 Importance of Synthetic Data"}),"\n",(0,i.jsx)(n.p,{children:"Generating real-world labeled datasets can be:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Expensive"}),": Cameras, LiDAR, and IMU sensors are costly."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Time-consuming"}),": Manual labeling of images and 3D data requires extensive effort."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Error-prone"}),": Human labeling may introduce inconsistencies."]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["Synthetic data solves these issues by providing ",(0,i.jsx)(n.strong,{children:"perfect labels"})," with precise ground truth for ",(0,i.jsx)(n.strong,{children:"segmentation masks, depth maps, and keypoints"}),", accelerating model development and training."]}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Definition:"})," ",(0,i.jsx)(n.em,{children:"Synthetic Dataset"}),": A computer-generated dataset that simulates real-world sensor data with exact ground truth annotations for machine learning."]}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"32-types-of-synthetic-labels",children:"3.2 Types of Synthetic Labels"}),"\n",(0,i.jsx)(n.p,{children:"Isaac Sim supports the following types of synthetic labels:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Segmentation Masks:"})," Assign each pixel a class label, such as ",(0,i.jsx)(n.code,{children:"robot"}),", ",(0,i.jsx)(n.code,{children:"human"}),", ",(0,i.jsx)(n.code,{children:"floor"}),", or ",(0,i.jsx)(n.code,{children:"obstacle"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Depth Maps:"})," Generate per-pixel depth information from the camera or sensor perspective."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"2D/3D Keypoints:"})," Annotate specific points on robots, humans, or objects for pose estimation."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Bounding Boxes:"})," Define 2D or 3D bounding volumes around objects for detection tasks."]}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"33-generating-synthetic-data-in-isaac-sim",children:"3.3 Generating Synthetic Data in Isaac Sim"}),"\n",(0,i.jsx)(n.h3,{id:"step-1-setup-simulation-scene",children:"Step 1: Setup Simulation Scene"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Load the humanoid robot model and environment assets."}),"\n",(0,i.jsx)(n.li,{children:"Position cameras, LiDAR, or other sensors."}),"\n",(0,i.jsx)(n.li,{children:"Configure lighting and materials for realistic rendering."}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"step-2-enable-ground-truth-output",children:"Step 2: Enable Ground Truth Output"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Enable labeling and ground truth pipelines in Isaac Sim:"}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# Example: Enable segmentation and depth outputs\r\nfrom omni.isaac.synthetic_utils import SyntheticDataGenerator\r\n\r\ndata_gen = SyntheticDataGenerator(stage)\r\ndata_gen.enable_segmentation(True)\r\ndata_gen.enable_depth(True)\r\ndata_gen.enable_keypoints(True)\n"})}),"\n",(0,i.jsx)(n.h3,{id:"step-3-record-synthetic-data",children:"Step 3: Record Synthetic Data"}),"\n",(0,i.jsx)(n.p,{children:"Once the simulation scene is configured with humanoid robots, sensors, and environment assets:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Simulate robot movements, human avatars, or dynamic objects."}),"\n",(0,i.jsx)(n.li,{children:"Capture sensor outputs at desired frame rates."}),"\n",(0,i.jsxs)(n.li,{children:["Record ",(0,i.jsx)(n.strong,{children:"segmentation masks, depth maps, 2D/3D keypoints, and bounding boxes"}),"."]}),"\n"]}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Example:"})," Capture frames while the humanoid robot performs walking, reaching, or interaction tasks to generate labeled datasets for perception model training."]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"step-4-export-dataset",children:"Step 4: Export Dataset"}),"\n",(0,i.jsx)(n.p,{children:"After recording:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Export datasets in standard formats suitable for machine learning:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"COCO"})," for object detection/segmentation."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"KITTI"})," for depth and point cloud data."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Pascal VOC"})," for classification and segmentation."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"NumPy arrays"})," for custom pipelines."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:["Ensure ",(0,i.jsx)(n.strong,{children:"labels are synchronized"})," with sensor outputs and stored consistently."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Validate dataset integrity by sampling frames and verifying annotations."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Tip:"})," Use ",(0,i.jsx)(n.strong,{children:"domain randomization"})," (varying lighting, textures, and camera angles) to improve model generalization in real-world deployment."]}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"34-applications-of-synthetic-data",children:"3.4 Applications of Synthetic Data"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Perception Model Training:"})," Train neural networks for object detection, segmentation, or pose estimation."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Reinforcement Learning:"})," Provide synthetic visual observations for learning robotic policies."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Sim-to-Real Transfer:"})," Fine-tune models trained on synthetic data using smaller real-world datasets for better real-world performance."]}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"35-best-practices",children:"3.5 Best Practices"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Diversity:"})," Include varied scenes, lighting conditions, and object positions."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Resolution:"})," Match sensor resolution to target deployment hardware."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Consistency:"})," Maintain labeling conventions across frames for reliable training."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Validation:"})," Compare synthetic outputs against real-world samples to verify realism."]}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"36-summary",children:"3.6 Summary"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Synthetic data accelerates ",(0,i.jsx)(n.strong,{children:"training of perception models"})," without requiring physical robots."]}),"\n",(0,i.jsxs)(n.li,{children:["Isaac Sim enables automated capture of ",(0,i.jsx)(n.strong,{children:"segmentation, depth, and keypoints"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:["Properly generated synthetic datasets improve ",(0,i.jsx)(n.strong,{children:"sim-to-real transfer"})," and model robustness."]}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"37-learning-outcomes",children:"3.7 Learning Outcomes"}),"\n",(0,i.jsx)(n.p,{children:"After completing these steps, students will be able to:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Record synthetic data for humanoid robots with multiple sensors."}),"\n",(0,i.jsxs)(n.li,{children:["Export labeled datasets in ",(0,i.jsx)(n.strong,{children:"standard formats"})," ready for ML training."]}),"\n",(0,i.jsxs)(n.li,{children:["Apply ",(0,i.jsx)(n.strong,{children:"domain randomization"})," and best practices to increase dataset quality."]}),"\n",(0,i.jsxs)(n.li,{children:["Integrate synthetic datasets into ML pipelines for ",(0,i.jsx)(n.strong,{children:"perception and control tasks"}),"."]}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"references",children:"References"}),"\n",(0,i.jsxs)(n.p,{children:["[1] NVIDIA, \u201cIsaac Sim Documentation \u2013 Synthetic Data Generation,\u201d ",(0,i.jsx)(n.em,{children:(0,i.jsx)(n.a,{href:"https://developer.nvidia.com/isaac-sim",children:"https://developer.nvidia.com/isaac-sim"})}),"."]}),"\n",(0,i.jsxs)(n.p,{children:["[2] Tobin, J., et al., \u201cDomain Randomization for Sim-to-Real Transfer,\u201d ",(0,i.jsx)(n.em,{children:"IEEE/RSJ IROS"}),", 2017."]}),"\n",(0,i.jsxs)(n.p,{children:["[3] Richter, S. R., et al., \u201cPlaying for Data: Ground Truth from Computer Games,\u201d ",(0,i.jsx)(n.em,{children:"ECCV"}),", 2016."]}),"\n",(0,i.jsxs)(n.p,{children:["[4] ROS 2 Documentation, \u201cIsaac ROS Integration for Perception,\u201d ",(0,i.jsx)(n.em,{children:(0,i.jsx)(n.a,{href:"https://docs.ros.org",children:"https://docs.ros.org"})}),"."]}),"\n",(0,i.jsxs)(n.p,{children:["[5] Siciliano, B., et al., ",(0,i.jsx)(n.em,{children:"Springer Handbook of Robotics"}),", 2nd ed., Springer, 2016."]}),"\n",(0,i.jsx)(n.hr,{})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}}}]);