"use strict";(globalThis.webpackChunkhumanoid_robotics_book=globalThis.webpackChunkhumanoid_robotics_book||[]).push([[6535],{7407:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>l,contentTitle:()=>t,default:()=>m,frontMatter:()=>o,metadata:()=>r,toc:()=>c});const r=JSON.parse('{"id":"digital-twin/sensor-simulation","title":"Sensor Simulation","description":"Chapter 05:","source":"@site/docs/02-digital-twin/05-sensor-simulation.md","sourceDirName":"02-digital-twin","slug":"/digital-twin/sensor-simulation","permalink":"/hackathon-1/docs/digital-twin/sensor-simulation","draft":false,"unlisted":false,"editUrl":"https://github.com/FareaaFaisal/hackathon-1/edit/main/docs/02-digital-twin/05-sensor-simulation.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"sidebar_position":5,"title":"Sensor Simulation"},"sidebar":"tutorialSidebar","previous":{"title":"Physics Simulation (Rigid Bodies, Collisions, Gravity)","permalink":"/hackathon-1/docs/digital-twin/physics-simulation"},"next":{"title":"Unity Integration","permalink":"/hackathon-1/docs/digital-twin/unity-integration"}}');var s=i(4848),a=i(8453);const o={sidebar_position:5,title:"Sensor Simulation"},t=void 0,l={},c=[{value:"Chapter 05:",id:"chapter-05",level:2},{value:"5.1 LiDAR Simulation",id:"51-lidar-simulation",level:2},{value:"5.2 RGB Camera Simulation",id:"52-rgb-camera-simulation",level:2},{value:"5.3 Depth Camera Simulation",id:"53-depth-camera-simulation",level:2},{value:"5.4 IMU Simulation",id:"54-imu-simulation",level:2},{value:"5.5 Accessing and Using Sensor Data",id:"55-accessing-and-using-sensor-data",level:2},{value:"5.6 Summary",id:"56-summary",level:2},{value:"5.7 Learning Outcomes",id:"57-learning-outcomes",level:2},{value:"References",id:"references",level:2}];function d(n){const e={a:"a",blockquote:"blockquote",code:"code",h2:"h2",hr:"hr",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...n.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(e.h2,{id:"chapter-05",children:"Chapter 05:"}),"\n",(0,s.jsxs)(e.p,{children:["Sensors are critical for humanoid robots to perceive their environment. In simulation, we can ",(0,s.jsx)(e.strong,{children:"emulate LiDAR, RGB cameras, Depth cameras, and IMUs"})," to test perception algorithms before deployment. Using Gazebo and ROS 2, simulated sensors publish data to ",(0,s.jsx)(e.strong,{children:"ROS 2 topics"}),", allowing developers to integrate perception pipelines and verify functionality in a safe environment."]}),"\n",(0,s.jsx)(e.hr,{}),"\n",(0,s.jsx)(e.h2,{id:"51-lidar-simulation",children:"5.1 LiDAR Simulation"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"LiDAR (Light Detection and Ranging)"})," sensors provide 2D or 3D point clouds representing the environment. In Gazebo, LiDAR is defined as a sensor plugin in the robot's URDF or SDF file:"]}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-xml",children:'<sensor name="lidar_sensor" type="ray">\r\n  <pose>0 0 1 0 0 0</pose>\r\n  <ray>\r\n    <scan>\r\n      <horizontal>\r\n        <samples>640</samples>\r\n        <resolution>1</resolution>\r\n        <min_angle>-1.5708</min_angle>\r\n        <max_angle>1.5708</max_angle>\r\n      </horizontal>\r\n    </scan>\r\n    <range>\r\n      <min>0.1</min>\r\n      <max>30.0</max>\r\n      <resolution>0.01</resolution>\r\n    </range>\r\n  </ray>\r\n</sensor>\n'})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Publishes sensor_msgs/msg/LaserScan data to a ROS 2 topic (e.g., /lidar_scan)."}),"\n",(0,s.jsx)(e.li,{children:"Used for obstacle detection, mapping, and navigation."}),"\n"]}),"\n",(0,s.jsxs)(e.blockquote,{children:["\n",(0,s.jsx)(e.p,{children:"Tip: Adjust the number of samples and scan resolution for performance vs. fidelity trade-offs."}),"\n"]}),"\n",(0,s.jsx)(e.hr,{}),"\n",(0,s.jsx)(e.h2,{id:"52-rgb-camera-simulation",children:"5.2 RGB Camera Simulation"}),"\n",(0,s.jsx)(e.p,{children:"RGB cameras simulate standard color cameras used for vision tasks. In Gazebo:"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-bash",children:'<sensor name="rgb_camera" type="camera">\r\n  <camera>\r\n    <image>\r\n      <width>640</width>\r\n      <height>480</height>\r\n      <format>R8G8B8</format>\r\n    </image>\r\n    <clip>\r\n      <near>0.1</near>\r\n      <far>50.0</far>\r\n    </clip>\r\n  </camera>\r\n</sensor>\n'})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Publishes sensor_msgs/msg/Image on a ROS 2 topic (e.g., /camera/rgb/image_raw)."}),"\n",(0,s.jsx)(e.li,{children:"Commonly used for object detection, visual SLAM, and human-robot interaction."}),"\n"]}),"\n",(0,s.jsxs)(e.blockquote,{children:["\n",(0,s.jsx)(e.p,{children:"Tip: Ensure the camera frame aligns correctly with the robot's body for accurate perception."}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"53-depth-camera-simulation",children:"5.3 Depth Camera Simulation"}),"\n",(0,s.jsx)(e.p,{children:"Depth cameras provide 3D distance information for each pixel. Useful for obstacle avoidance and 3D reconstruction:"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-bash",children:'<sensor name="depth_camera" type="depth">\r\n  <camera>\r\n    <image>\r\n      <width>640</width>\r\n      <height>480</height>\r\n      <format>R8G8B8</format>\r\n    </image>\r\n  </camera>\r\n</sensor>\n'})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Publishes sensor_msgs/msg/Image or sensor_msgs/msg/PointCloud2 depending on configuration."}),"\n",(0,s.jsx)(e.li,{children:"Compatible with ROS 2 perception libraries like PCL for processing point clouds."}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"54-imu-simulation",children:"5.4 IMU Simulation"}),"\n",(0,s.jsx)(e.p,{children:"IMU (Inertial Measurement Unit) provides orientation, angular velocity, and linear acceleration. Used for balance, motion control, and state estimation."}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-bash",children:'<sensor name="imu_sensor" type="imu">\r\n  <imu>\r\n    <angular_velocity>\r\n      <x>0</x>\r\n      <y>0</y>\r\n      <z>0</z>\r\n    </angular_velocity>\r\n    <linear_acceleration>\r\n      <x>0</x>\r\n      <y>0</y>\r\n      <z>-9.81</z>\r\n    </linear_acceleration>\r\n  </imu>\r\n</sensor>\n'})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Publishes sensor_msgs/msg/Imu on a ROS 2 topic (e.g., /imu/data)."}),"\n",(0,s.jsx)(e.li,{children:"Critical for humanoid balance, walking, and motion planning."}),"\n"]}),"\n",(0,s.jsxs)(e.blockquote,{children:["\n",(0,s.jsx)(e.p,{children:"Tip: Validate IMU orientation with respect to the robot base frame to avoid incorrect pose estimation."}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"55-accessing-and-using-sensor-data",children:"5.5 Accessing and Using Sensor Data"}),"\n",(0,s.jsx)(e.p,{children:"Once sensors are configured, you can read the data in ROS 2 using subscriber nodes:"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-bash",children:"import rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import LaserScan\r\n\r\nclass LidarReader(Node):\r\n    def __init__(self):\r\n        super().__init__('lidar_reader')\r\n        self.subscription = self.create_subscription(\r\n            LaserScan,\r\n            '/lidar_scan',\r\n            self.listener_callback,\r\n            10\r\n        )\r\n\r\n    def listener_callback(self, msg):\r\n        self.get_logger().info(f\"Received {len(msg.ranges)} points\")\r\n\r\nrclpy.init()\r\nnode = LidarReader()\r\nrclpy.spin(node)\n"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Same approach applies for RGB, Depth, and IMU topics."}),"\n",(0,s.jsx)(e.li,{children:"Enables integration with perception pipelines, SLAM, and control algorithms."}),"\n"]}),"\n",(0,s.jsx)(e.hr,{}),"\n",(0,s.jsx)(e.h2,{id:"56-summary",children:"5.6 Summary"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Simulated sensors replicate real-world perception in Gazebo."}),"\n",(0,s.jsx)(e.li,{children:"LiDAR, RGB, Depth, and IMU sensors provide data on environment, obstacles, and robot motion."}),"\n",(0,s.jsx)(e.li,{children:"ROS 2 topics allow easy integration of sensor data into perception and control pipelines."}),"\n",(0,s.jsx)(e.li,{children:"Accurate configuration ensures reliable simulation for testing humanoid robotics tasks."}),"\n"]}),"\n",(0,s.jsx)(e.hr,{}),"\n",(0,s.jsx)(e.h2,{id:"57-learning-outcomes",children:"5.7 Learning Outcomes"}),"\n",(0,s.jsx)(e.p,{children:"After completing this chapter, students will be able to:"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Configure and integrate LiDAR, RGB cameras, Depth cameras, and IMUs in Gazebo simulations."}),"\n",(0,s.jsx)(e.li,{children:"Access and interpret simulated sensor data via ROS 2 topics."}),"\n",(0,s.jsx)(e.li,{children:"Apply sensor data for perception tasks, including mapping, obstacle detection, and state estimation."}),"\n",(0,s.jsx)(e.li,{children:"Validate that simulated sensors reflect realistic behavior for humanoid robots."}),"\n"]}),"\n",(0,s.jsx)(e.hr,{}),"\n",(0,s.jsx)(e.h2,{id:"references",children:"References"}),"\n",(0,s.jsx)(e.p,{children:"[1] Koenig, N., Howard, A., \u201cDesign and Use Paradigms for Gazebo, an Open-Source Multi-Robot Simulator,\u201d IEEE/RSJ IROS, 2004."}),"\n",(0,s.jsxs)(e.p,{children:["[2] ROS 2 Documentation, \u201cUsing Sensors in Gazebo,\u201d ",(0,s.jsx)(e.a,{href:"https://docs.ros.org/en/humble/Tutorials/Simulation/Gazebo-Sensors.html",children:"https://docs.ros.org/en/humble/Tutorials/Simulation/Gazebo-Sensors.html"})]}),"\n",(0,s.jsxs)(e.p,{children:["[3] PCL Documentation, \u201cPoint Cloud Library,\u201d ",(0,s.jsx)(e.a,{href:"https://pointclouds.org/",children:"https://pointclouds.org/"})]}),"\n",(0,s.jsx)(e.p,{children:"[4] Siciliano, B., et al., Springer Handbook of Robotics, 2nd ed., Springer, 2016."}),"\n",(0,s.jsxs)(e.p,{children:["[5] Open Source Robotics Foundation, \u201cGazebo Sensor Plugins,\u201d ",(0,s.jsx)(e.a,{href:"http://gazebosim.org/tutorials?tut=plugins_sensors",children:"http://gazebosim.org/tutorials?tut=plugins_sensors"})]}),"\n",(0,s.jsx)(e.hr,{})]})}function m(n={}){const{wrapper:e}={...(0,a.R)(),...n.components};return e?(0,s.jsx)(e,{...n,children:(0,s.jsx)(d,{...n})}):d(n)}},8453:(n,e,i)=>{i.d(e,{R:()=>o,x:()=>t});var r=i(6540);const s={},a=r.createContext(s);function o(n){const e=r.useContext(a);return r.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function t(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(s):n.components||s:o(n.components),r.createElement(a.Provider,{value:e},n.children)}}}]);