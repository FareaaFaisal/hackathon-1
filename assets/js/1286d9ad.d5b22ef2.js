"use strict";(globalThis.webpackChunkhumanoid_robotics_book=globalThis.webpackChunkhumanoid_robotics_book||[]).push([[1822],{3577:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>r,contentTitle:()=>l,default:()=>u,frontMatter:()=>a,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"vision-language-action/vlm","title":"Vision-Language Models","description":"Vision-Language Models","source":"@site/docs/04-vision-language-action/04-vlm.md","sourceDirName":"04-vision-language-action","slug":"/vision-language-action/vlm","permalink":"/hackathon-1/docs/vision-language-action/vlm","draft":false,"unlisted":false,"editUrl":"https://github.com/FareaaFaisal/hackathon-1/edit/main/docs/04-vision-language-action/04-vlm.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"sidebar_position":4,"title":"Vision-Language Models"},"sidebar":"tutorialSidebar","previous":{"title":"LLM Planning","permalink":"/hackathon-1/docs/vision-language-action/llm-planning"},"next":{"title":"Multimodal Pipeline","permalink":"/hackathon-1/docs/vision-language-action/multimodal-pipeline"}}');var o=i(4848),t=i(8453);const a={sidebar_position:4,title:"Vision-Language Models"},l=void 0,r={},c=[{value:"Vision-Language Models",id:"vision-language-models",level:2},{value:"Requirements:",id:"requirements",level:3},{value:"Success Criteria:",id:"success-criteria",level:3}];function d(n){const e={h2:"h2",h3:"h3",hr:"hr",li:"li",p:"p",strong:"strong",ul:"ul",...(0,t.R)(),...n.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(e.h2,{id:"vision-language-models",children:"Vision-Language Models"}),"\n",(0,o.jsx)(e.p,{children:"This chapter covers scene understanding from vision using Vision-Language Models (VLMs)."}),"\n",(0,o.jsx)(e.h3,{id:"requirements",children:"Requirements:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Scene understanding from vision."}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"success-criteria",children:"Success Criteria:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Robot identifies objects on command."}),"\n"]}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Learning Outcomes"}),":"]}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Understand the architecture and capabilities of Vision-Language Models (VLMs)."}),"\n",(0,o.jsx)(e.li,{children:"Implement VLMs for robust scene understanding and object identification in robotic systems."}),"\n",(0,o.jsx)(e.li,{children:"Enable robots to identify and interact with objects based on natural language commands."}),"\n"]})]})}function u(n={}){const{wrapper:e}={...(0,t.R)(),...n.components};return e?(0,o.jsx)(e,{...n,children:(0,o.jsx)(d,{...n})}):d(n)}},8453:(n,e,i)=>{i.d(e,{R:()=>a,x:()=>l});var s=i(6540);const o={},t=s.createContext(o);function a(n){const e=s.useContext(t);return s.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function l(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(o):n.components||o:a(n.components),s.createElement(t.Provider,{value:e},n.children)}}}]);