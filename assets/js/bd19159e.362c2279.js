"use strict";(globalThis.webpackChunkhumanoid_robotics_book=globalThis.webpackChunkhumanoid_robotics_book||[]).push([[2419],{8311:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>c,contentTitle:()=>a,default:()=>h,frontMatter:()=>o,metadata:()=>s,toc:()=>l});const s=JSON.parse('{"id":"vision-language-action/whisper-input","title":"Whisper for Speech Input","description":"Chapter 2:","source":"@site/docs/04-vision-language-action/02-whisper-input.md","sourceDirName":"04-vision-language-action","slug":"/vision-language-action/whisper-input","permalink":"/hackathon-1/docs/vision-language-action/whisper-input","draft":false,"unlisted":false,"editUrl":"https://github.com/FareaaFaisal/hackathon-1/edit/main/docs/04-vision-language-action/02-whisper-input.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2,"title":"Whisper for Speech Input"},"sidebar":"tutorialSidebar","previous":{"title":"Introduction to VLA Robotics","permalink":"/hackathon-1/docs/vision-language-action/vla-intro"},"next":{"title":"LLM Planning","permalink":"/hackathon-1/docs/vision-language-action/llm-planning"}}');var r=i(4848),t=i(8453);const o={sidebar_position:2,title:"Whisper for Speech Input"},a=void 0,c={},l=[{value:"Chapter 2:",id:"chapter-2",level:2},{value:"2.1 Overview of Whisper",id:"21-overview-of-whisper",level:2},{value:"2.2 Installation and Setup",id:"22-installation-and-setup",level:2},{value:"2.3 Capturing Voice Commands",id:"23-capturing-voice-commands",level:2},{value:"2.4 Integration with Robotics",id:"24-integration-with-robotics",level:2},{value:"2.5 Best Practices",id:"25-best-practices",level:2},{value:"2.6 Applications",id:"26-applications",level:2},{value:"2.7 Summary",id:"27-summary",level:2},{value:"2.8 Learning Outcomes",id:"28-learning-outcomes",level:2},{value:"References",id:"references",level:2}];function d(n){const e={a:"a",blockquote:"blockquote",br:"br",code:"code",em:"em",h2:"h2",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...n.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(e.h2,{id:"chapter-2",children:"Chapter 2:"}),"\n",(0,r.jsxs)(e.p,{children:["Speech is a natural mode of communication for humans, and enabling robots to understand spoken instructions enhances ",(0,r.jsx)(e.strong,{children:"human-robot interaction"}),". OpenAI's ",(0,r.jsx)(e.strong,{children:"Whisper model"})," provides a robust framework for converting audio input into text, forming the foundation of a ",(0,r.jsx)(e.strong,{children:"voice-to-text pipeline"})," in robotics."]}),"\n",(0,r.jsx)(e.hr,{}),"\n",(0,r.jsx)(e.h2,{id:"21-overview-of-whisper",children:"2.1 Overview of Whisper"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["Whisper is a ",(0,r.jsx)(e.strong,{children:"state-of-the-art automatic speech recognition (ASR) model"})," trained on diverse datasets."]}),"\n",(0,r.jsx)(e.li,{children:"It provides high accuracy across multiple languages and accents."}),"\n",(0,r.jsxs)(e.li,{children:["Whisper can be integrated into robotics systems to process ",(0,r.jsx)(e.strong,{children:"real-time voice commands"})," for humanoid or service robots."]}),"\n"]}),"\n",(0,r.jsxs)(e.blockquote,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Definition:"})," ",(0,r.jsx)(e.em,{children:"Whisper Model"}),": A deep learning-based speech recognition system capable of transcribing spoken language into text for downstream robotic or AI tasks."]}),"\n"]}),"\n",(0,r.jsx)(e.hr,{}),"\n",(0,r.jsx)(e.h2,{id:"22-installation-and-setup",children:"2.2 Installation and Setup"}),"\n",(0,r.jsxs)(e.ol,{children:["\n",(0,r.jsx)(e.li,{children:(0,r.jsx)(e.strong,{children:"Install Python and Dependencies:"})}),"\n"]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-bash",children:"pip install openai-whisper\r\npip install torch torchaudio\n"})}),"\n",(0,r.jsxs)(e.ol,{start:"2",children:["\n",(0,r.jsx)(e.li,{children:(0,r.jsx)(e.strong,{children:"Verify Installation:"})}),"\n"]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-bash",children:"whisper --help\n"})}),"\n",(0,r.jsxs)(e.ol,{start:"3",children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Optional GPU Support:"})," Ensure PyTorch is configured with CUDA for faster transcription."]}),"\n"]}),"\n",(0,r.jsx)(e.hr,{}),"\n",(0,r.jsx)(e.h2,{id:"23-capturing-voice-commands",children:"2.3 Capturing Voice Commands"}),"\n",(0,r.jsxs)(e.ol,{children:["\n",(0,r.jsx)(e.li,{children:(0,r.jsx)(e.strong,{children:"Audio Input Capture:"})}),"\n"]}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Use Python libraries such as sounddevice or pyaudio to record microphone input."}),"\n",(0,r.jsx)(e.li,{children:"Ensure sampling rate is compatible with Whisper (typically 16 kHz or 44.1 kHz)."}),"\n"]}),"\n",(0,r.jsxs)(e.ol,{start:"2",children:["\n",(0,r.jsx)(e.li,{children:(0,r.jsx)(e.strong,{children:"Transcription Pipeline:"})}),"\n"]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'import whisper\r\n\r\nmodel = whisper.load_model("base")\r\nresult = model.transcribe("input_audio.wav")\r\nprint(result["text"])\n'})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"The pipeline converts raw audio into clean text output, which can be fed into language understanding modules."}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"24-integration-with-robotics",children:"2.4 Integration with Robotics"}),"\n",(0,r.jsx)(e.p,{children:"Once speech is transcribed into text, it must be interpreted and executed by the robot. Integration with robotic systems involves several components:"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Command Parsing:"})," Extract action verbs, objects, and context from the transcribed text using NLP or pattern-matching techniques."]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Planner or LLM Interface:"})," Feed parsed commands into a VLA (Vision-Language-Action) pipeline or ROS 2 nodes for planning and execution."]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Real-Time Execution:"})," Continuously monitor incoming audio, transcribe it, and execute commands without delays to maintain natural interaction."]}),"\n"]}),"\n",(0,r.jsxs)(e.blockquote,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Example:"}),(0,r.jsx)(e.br,{}),"\n","User says: ",(0,r.jsx)(e.em,{children:"\u201cPick up the red cube.\u201d"})]}),"\n",(0,r.jsxs)(e.ol,{children:["\n",(0,r.jsxs)(e.li,{children:["Whisper transcribes audio \u2192 ",(0,r.jsx)(e.code,{children:'"Pick up the red cube"'})]}),"\n",(0,r.jsxs)(e.li,{children:["NLP module parses intent \u2192 action: ",(0,r.jsx)(e.em,{children:"pick"}),", object: ",(0,r.jsx)(e.em,{children:"red cube"})]}),"\n",(0,r.jsx)(e.li,{children:"VLA/ROS pipeline generates motion commands \u2192 robot executes pick-and-place"}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(e.hr,{}),"\n",(0,r.jsx)(e.h2,{id:"25-best-practices",children:"2.5 Best Practices"}),"\n",(0,r.jsx)(e.p,{children:"To ensure a robust voice-to-text and action pipeline:"}),"\n",(0,r.jsxs)(e.ol,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Noise Reduction:"})," Use directional microphones and audio filters to minimize background noise."]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Continuous Listening:"})," Implement streaming audio input for uninterrupted command recognition."]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Feedback Loop:"})," Provide visual or auditory confirmation to the user that the command was understood."]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Contextual Understanding:"})," Pair Whisper output with LLMs or rule-based systems for disambiguation."]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Testing Across Environments:"})," Validate accuracy with different speakers, accents, and room conditions."]}),"\n"]}),"\n",(0,r.jsx)(e.hr,{}),"\n",(0,r.jsx)(e.h2,{id:"26-applications",children:"2.6 Applications"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Voice-Controlled Humanoids:"})," Enable natural command execution for service robots."]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Interactive Assistants:"})," Industrial, healthcare, and research applications."]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Multimodal VLA Integration:"})," Combine speech with vision and action for complex tasks."]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Training and Debugging:"})," Generate logs of recognized commands for system validation."]}),"\n"]}),"\n",(0,r.jsx)(e.hr,{}),"\n",(0,r.jsx)(e.h2,{id:"27-summary",children:"2.7 Summary"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["Whisper provides ",(0,r.jsx)(e.strong,{children:"robust speech-to-text conversion"})," for robotic systems."]}),"\n",(0,r.jsx)(e.li,{children:"Integrating Whisper with VLA or ROS pipelines allows robots to act on natural language commands."}),"\n",(0,r.jsx)(e.li,{children:"Following best practices ensures reliable performance across real-world conditions."}),"\n"]}),"\n",(0,r.jsx)(e.hr,{}),"\n",(0,r.jsx)(e.h2,{id:"28-learning-outcomes",children:"2.8 Learning Outcomes"}),"\n",(0,r.jsx)(e.p,{children:"After completing this section, students will be able to:"}),"\n",(0,r.jsxs)(e.ol,{children:["\n",(0,r.jsx)(e.li,{children:"Integrate Whisper transcriptions with robotic planners or VLA pipelines."}),"\n",(0,r.jsx)(e.li,{children:"Parse and interpret commands for humanoid action execution."}),"\n",(0,r.jsx)(e.li,{children:"Implement real-time voice-to-action systems."}),"\n",(0,r.jsx)(e.li,{children:"Apply best practices for noise reduction, feedback, and contextual understanding."}),"\n",(0,r.jsx)(e.li,{children:"Evaluate and optimize speech-based control for reliable human-robot interaction."}),"\n"]}),"\n",(0,r.jsx)(e.hr,{}),"\n",(0,r.jsx)(e.h2,{id:"references",children:"References"}),"\n",(0,r.jsxs)(e.p,{children:["[1] OpenAI, \u201cWhisper: Robust Speech Recognition,\u201d ",(0,r.jsx)(e.em,{children:"arXiv preprint"}),", 2022."]}),"\n",(0,r.jsxs)(e.p,{children:["[2] Hannun, A., et al., \u201cDeep Speech: Scaling Speech Recognition,\u201d ",(0,r.jsx)(e.em,{children:"arXiv preprint"}),", 2014."]}),"\n",(0,r.jsxs)(e.p,{children:["[3] Chen, L., et al., \u201cLanguage-Conditioned Imitation Learning for Robot Manipulation,\u201d ",(0,r.jsx)(e.em,{children:"IEEE Robotics and Automation Letters"}),", 2021."]}),"\n",(0,r.jsxs)(e.p,{children:["[4] Shridhar, M., et al., \u201cVIMA: General Robot Manipulation with Multi-Modal Prompts,\u201d ",(0,r.jsx)(e.em,{children:"ICRA"}),", 2022."]}),"\n",(0,r.jsxs)(e.p,{children:["[5] NVIDIA, \u201cSpeech Recognition for Robotics Applications,\u201d ",(0,r.jsx)(e.em,{children:(0,r.jsx)(e.a,{href:"https://developer.nvidia.com/robotics",children:"https://developer.nvidia.com/robotics"})}),"."]}),"\n",(0,r.jsx)(e.hr,{})]})}function h(n={}){const{wrapper:e}={...(0,t.R)(),...n.components};return e?(0,r.jsx)(e,{...n,children:(0,r.jsx)(d,{...n})}):d(n)}},8453:(n,e,i)=>{i.d(e,{R:()=>o,x:()=>a});var s=i(6540);const r={},t=s.createContext(r);function o(n){const e=s.useContext(t);return s.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function a(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(r):n.components||r:o(n.components),s.createElement(t.Provider,{value:e},n.children)}}}]);